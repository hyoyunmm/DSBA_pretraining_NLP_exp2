# DSBA_pretraining_NLP_exp2

## 목표 1. gradient accumulation 적용, best EBS 찾기

### 1-1. 기본 setup 및 비교대상(model, EBS)
  - lr, bs, optimizer: 5e-5, 16, adam
  - accum_steps: 16, 32, 64
  - EBS(업데이트되는 기준 갯수 = batch_size * accum_steps): 256 vs. 512 vs. 1024

- 결과: bert(256), modernbert(512)
  
| Name               | data.batch_size | accum_steps | optimizer | lr      | train/acc | train/loss | val/acc | val/loss | test/acc | test/loss |
|--------------------|-----------------|-------------|-----------|---------|-----------|------------|---------|----------|----------|-----------|
| **bert_ebs256**        | 16              | 16          | adam      | 0.00005 | 0.8125    | 0.4442     | 0.7834  | 0.4706   | 0.8590   | 0.3186    |
| bert_ebs512        | 16              | 32          | adam      | 0.00005 | 0.8438    | 0.3463     | 0.8378  | 0.3742   | 0.8582   | 0.3207    |
| bert_ebs1024       | 16              | 64          | adam      | 0.00005 | 0.8906    | 0.2660     | **0.8676**  | 0.3071   | **0.8688**   | 0.3014    |
| modernbert_ebs256  | 16              | 16          | adam      | 0.00005 | 0.9219    | 0.2016     | 0.8962  | 0.2482   | 0.8994   | 0.2460    |
| **modernbert_ebs512**  | 16              | 32          | adam      | 0.00005 | 0.9375    | 0.1574     | **0.9076**  | 0.2249   | **0.9054**   | 0.2308    |
| modernbert_ebs1024 | 16              | 64          | adam      | 0.00005 | 0.9531    | 0.1282     | 0.9050  | 0.2392   | 0.9040   | 0.2420    |


  
## 목표 2. accelerator로 gradient accumulation 적용, best EBS
### 2-1. 기본 setup 동일하게 실험
- 결과: bert(256), modernbert(512)
![Uploading W&B Chart 2025. 8. 25. 오후 2_16_18.png…](./Uploading W&B Chart 2025. 8. 25. 오후 2_16_18.png)

![Uploading W&B Chart 2025. 8. 25. 오후 2_16_18.png…](./Uploading W&B Chart 2025. 8. 25. 오후 2_16_18.png)

| Name               | train/acc | train/loss | val/acc | val/loss | test/acc | test/loss |
|--------------------|-----------|------------|---------|----------|----------|-----------|
| bert_ebs256        | 0.7762    | 0.0292     | 0.6018  | 0.8700   | **0.8138**   | 0.4088    |
| bert_ebs512        | 0.7745    | 0.0147     | **0.7818**  | 0.4537   | 0.8122   | 0.4079    |
| bert_ebs1024       | –         | –          | 0.7750  | 0.4639   | 0.7812   | 0.4644    |
| modernbert_ebs256  | 0.8673    | 0.0192     | 0.8628  | 0.3136   | 0.8870   | 0.2716    |
| modernbert_ebs512  | 0.8792    | 0.0090     | **0.8846**  | 0.2742   | **0.8898**   | 0.2702    |
| modernbert_ebs1024 | –         | –          | 0.8792  | 0.2903   | 0.8840   | 0.2881    |

### 2-2. optimizer 변경(adamW) 후 비교

- 결과: bert(256), modernbert(512)
- 그런데... 뒤늦게 코드 오류 발견 -> 재실험 필요

| Name                   | train/acc | train/loss | val/acc | val/loss | test/acc | test/loss |
|------------------------|-----------|------------|---------|----------|----------|-----------|
| bert_ebs256_adamw      | 0.7762    | 0.0292     | 0.6018  | 0.8700   | **0.8138**   | 0.4088    |
| bert_ebs512_adamw      | 0.7745    | 0.0147     | **0.7818**  | 0.4537   | 0.8122   | 0.4079    |
| bert_ebs1024_adamw     | –         | –          | 0.7750  | 0.4639   | 0.7812   | 0.4644    |
| modernbert_ebs256_adamw| 0.8665    | 0.0193     | 0.8670  | 0.3032   | 0.8846   | 0.2739    |
| modernbert_ebs512_adamw| 0.8787    | 0.0090     | **0.8840**  | 0.2746   | **0.8884**   | 0.2712    |
| modernbert_ebs1024_adamw| –        | –          | 0.8764  | 0.2942   | 0.8812   | 0.2921    |


