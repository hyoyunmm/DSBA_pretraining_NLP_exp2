seed: 42

data:
  model_name: answerdotai/ModernBERT-base
  max_len: 128 # 256, 512
  train_frac: 0.8
  val_frac: 0.1
  test_frac: 0.1
  batch_size: 16 # 32, 64
  num_workers: 2


model:
  model_name: answerdotai/ModernBERT-base
  dropout: 0.1 # 정규화 정도 0.3
  pooling: cls # Vs. mean
  num_labels: 2
  freeze_backbone: false ## 

train_config:
  epochs: 5 ## 이후에 5로 수정
  lr: 5e-5
  # batch_size: 16             # per-device
  accum_steps: 16            # 16*16 = 256 (single GPU)
  weight_decay: 0.01
  warmup_ratio: 0.06 ##
  # gradient_checkpointing: true
  grad_clip: 1.0
  
  device: auto
  optimizer: adam
  scheduler: constant
  log_every: 50
  deterministic: false
  output_dir: outputs/modernbert_ebs256 ## 변경

logging:
  use_wandb: true
  project: nlp-bert-grad-accumulation
  run_name: modernbert_ebs256 ## 변경

